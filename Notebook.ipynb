{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7962, 25, 50) (7962, 25, 100) (7962,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "mouth_cascade = cv2.CascadeClassifier('haarcascade_mcs_mouth.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_mcs_eyepair_big.xml')\n",
    "\n",
    "def segmentor(image):\n",
    "    img = cv2.imread(image)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = img[y:y + h, x:x + w]\n",
    "        \n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.1, 3)\n",
    "        mouth = mouth_cascade.detectMultiScale(roi_gray, 1.1, 3)\n",
    "        \n",
    "        if len(eyes) >= 1 and len(mouth) >= 1:\n",
    "        \n",
    "            ex, ey, ew, eh = eyes[0]\n",
    "            mx, my, mw, mh = mouth[0]\n",
    "            \n",
    "            cropped_mouth = gray[y + my:y + my + mh, x + mx:x + mx + mw]\n",
    "            cropped_eyes = gray[y + ey:y + ey + eh, x + ex: x + ex + ew]\n",
    "\n",
    "            cropped_mouth = cv2.resize(cropped_mouth, (50, 25))\n",
    "            cropped_eyes = cv2.resize(cropped_eyes, (100, 25))\n",
    "\n",
    "            return [cropped_eyes, cropped_mouth]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "#====================\n",
    "y = []\n",
    "x_mouth = []\n",
    "x_eyes = []\n",
    "#   ADD JAFFE DATABASE FIRST\n",
    "f = open('emotions_jaffe_lakshay.txt')\n",
    "for line in f.read().split('\\r\\n'):\n",
    "    y.append(int(line))\n",
    "\n",
    "#   ADD 10K FACES NOW\n",
    "f = open('emotions_10k_0_3388.txt')\n",
    "for line in f.read().split('\\r\\n'):\n",
    "    y.append(int(line))\n",
    "\n",
    "\n",
    "f = open('emotions_10k_3389_6778.txt')\n",
    "for line in f.read().split('\\r\\n'):\n",
    "    y.append(int(line))\n",
    "\n",
    "\n",
    "f = open('emotions_10k_6779_10167.txt')\n",
    "for line in f.read().split('\\r\\n'):\n",
    "    y.append(int(line))\n",
    "\n",
    "y_labels = []\n",
    "i = 0\n",
    "#   COLLECT ALL JAFFE IMAGES\n",
    "files = sorted([f for f in listdir('jaffe') if isfile(join('jaffe', f)) and 'tiff' in f])\n",
    "for image in files:\n",
    "    result = segmentor('jaffe/' + image)\n",
    "    if result != None:\n",
    "        x_mouth.append(result[1])\n",
    "        x_eyes.append(result[0])\n",
    "        y_labels.append(y[i])\n",
    "    i += 1\n",
    "#   COLLECT ALL 10K IMAGES\n",
    "files = sorted([f for f in listdir('10kFaceImages') if isfile(join('10kFaceImages', f))])\n",
    "for image in files:\n",
    "    result = segmentor('10kFaceImages/' + image)\n",
    "    if result != None:\n",
    "        x_mouth.append(result[1])\n",
    "        x_eyes.append(result[0])\n",
    "        y_labels.append(y[i])\n",
    "    i += 1\n",
    "\n",
    "x_mouth = np.array(x_mouth)\n",
    "x_eyes = np.array(x_eyes)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "x_mouth.tofile('mouth.csv', sep=',')\n",
    "x_eyes.tofile('eyes.csv', sep=',')\n",
    "y_labels.tofile('labels.csv', sep=',')\n",
    "\n",
    "print x_mouth.shape, x_eyes.shape, y_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 25, 50) (2388, 25, 50)\n",
      "(5574, 25, 100) (2388, 25, 100)\n",
      "(5574,) (2388,)\n"
     ]
    }
   ],
   "source": [
    "#    Normalising the Data\n",
    "x_mouth = x_mouth.astype('float32') / 128.0 - 1\n",
    "x_eyes = x_eyes.astype('float32') / 128.0 - 1\n",
    "\n",
    "x_mouth_train = x_mouth[:5574]\n",
    "x_mouth_test = x_mouth[5574:]\n",
    "\n",
    "x_eyes_train = x_eyes[:5574]\n",
    "x_eyes_test = x_eyes[5574:]\n",
    "\n",
    "y_labels_train = y_labels[:5574]\n",
    "y_labels_test = y_labels[5574:]\n",
    "\n",
    "print x_mouth_train.shape, x_mouth_test.shape\n",
    "print x_eyes_train.shape, x_eyes_test.shape\n",
    "print y_labels_train.shape, y_labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 25, 50, 1) (5574, 4)\n",
      "(2388, 25, 50, 1) (2388, 4)\n",
      "(5574, 25, 100, 1) (5574, 4)\n",
      "(2388, 25, 100, 1) (2388, 4)\n"
     ]
    }
   ],
   "source": [
    "def reformat(data, Y):\n",
    "    xtrain = []\n",
    "    trainLen = data.shape[0]\n",
    "    for x in xrange(trainLen):\n",
    "        xtrain.append(data[x,:,:])\n",
    "    xtrain = np.asarray(xtrain)\n",
    "    Ytr=[]\n",
    "    for el in Y:\n",
    "        temp=np.zeros(4)\n",
    "        if el==1:\n",
    "            temp[0]=1\n",
    "        elif el==2:\n",
    "            temp[1]=1\n",
    "        elif el==3:\n",
    "            temp[2]=1\n",
    "        elif el==4:\n",
    "            temp[3]=1\n",
    "        Ytr.append(temp)\n",
    "    return xtrain, np.asarray(Ytr)\n",
    "\n",
    "mouth_train_data, mouth_train_labels = reformat(x_mouth_train, y_labels_train)\n",
    "mouth_test_data, mouth_test_labels = reformat(x_mouth_test, y_labels_test)\n",
    "eyes_train_data, eyes_train_labels = reformat(x_eyes_train, y_labels_train)\n",
    "eyes_test_data, eyes_test_labels = reformat(x_eyes_test, y_labels_test)\n",
    "mouth_train_data = np.reshape(mouth_train_data, (mouth_train_data.shape[0], mouth_train_data.shape[1], mouth_train_data.shape[2], 1))\n",
    "mouth_test_data = np.reshape(mouth_test_data, (mouth_test_data.shape[0], mouth_test_data.shape[1], mouth_test_data.shape[2], 1))\n",
    "eyes_train_data = np.reshape(eyes_train_data, (eyes_train_data.shape[0], eyes_train_data.shape[1], eyes_train_data.shape[2], 1))\n",
    "eyes_test_data = np.reshape(eyes_test_data, (eyes_test_data.shape[0], eyes_test_data.shape[1], eyes_test_data.shape[2], 1))\n",
    "print mouth_train_data.shape, mouth_train_labels.shape\n",
    "print mouth_test_data.shape, mouth_test_labels.shape\n",
    "print eyes_train_data.shape, eyes_train_labels.shape\n",
    "print eyes_test_data.shape, eyes_test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    Training CNN for mouth data\n",
    "image_size = 25\n",
    "width = 25\n",
    "height = 50\n",
    "height_eyes = 100\n",
    "channels = 1\n",
    "\n",
    "n_labels = 4\n",
    "patch = 5\n",
    "depth = 10\n",
    "hidden = 64\n",
    "dropout = 0.9375\n",
    "batch = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Minibatch loss at step 0: 1.377164\n",
      "Minibatch accuracy: 60.0%\n",
      "Minibatch loss at step 100: 0.562222\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 200: 1.039692\n",
      "Minibatch accuracy: 60.0%\n",
      "Minibatch loss at step 300: 0.824208\n",
      "Minibatch accuracy: 70.0%\n",
      "Minibatch loss at step 400: 1.076240\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 500: 0.733544\n",
      "Minibatch accuracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = tf.placeholder(tf.float32, shape=(batch, width, height_eyes, channels))\n",
    "tf_train_labels = tf.placeholder(tf.float32, shape=(batch, 4))\n",
    "\n",
    "tf_test_dataset = tf.constant(eyes_test_data)\n",
    "\n",
    "layer1_weights = tf.Variable(tf.truncated_normal([patch, patch, channels, depth], stddev=0.1))\n",
    "layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "layer2_weights = tf.Variable(tf.truncated_normal([patch, patch, depth, depth], stddev=0.1))\n",
    "layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "layer3_weights = tf.Variable(tf.truncated_normal([1750, 64], stddev=0.1))\n",
    "layer3_biases = tf.Variable(tf.constant(1.0, shape=[hidden]))\n",
    "\n",
    "layer4_weights = tf.Variable(tf.truncated_normal([hidden, n_labels], stddev=0.1))\n",
    "layer4_biases = tf.Variable(tf.constant(1.0, shape=[n_labels]))\n",
    "\n",
    "dropout = tf.placeholder(tf.float32)\n",
    "def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    #   Max Pool\n",
    "    hidden2 = tf.nn.max_pool(hidden1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #   Convolution 2 and RELU\n",
    "    conv2 = tf.nn.conv2d(hidden2, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden3 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    #   Max Pool\n",
    "    hidden4 = tf.nn.max_pool(hidden3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    shape = hidden4.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden4, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    hidden5 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    #   Dropout\n",
    "    dropout_layer = tf.nn.dropout(hidden5, 0.93)\n",
    "    final_mat = tf.matmul(dropout_layer, layer4_weights) + layer4_biases\n",
    "    return final_mat\n",
    "\n",
    "logits = model(tf_train_dataset)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print \"TRAINING\"\n",
    "    average = 0\n",
    "    for step in range(num_steps):\n",
    "        #   Constucting the batch from the data set\n",
    "        offset = (step * batch) % (eyes_train_labels.shape[0] - batch)\n",
    "        batch_data = eyes_train_data[offset:(offset + batch), :, :]\n",
    "        batch_labels = eyes_train_labels[offset:(offset + batch), :]\n",
    "        #   Dictionary to be fed to TensorFlow Session\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout: 0.93}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        #   Calculating the Accuracy of the predictions\n",
    "        accu = accuracy(predictions, batch_labels)\n",
    "        if (step % 100 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accu)\n",
    "        average += accu\n",
    "    print \"Average Training Accuracy : \", (average / num_steps)\n",
    "    print \"TESTING\"\n",
    "    average = 0\n",
    "    for step in range(num_steps):\n",
    "        #   Constucting the batch from the data set\n",
    "        offset = (step * batch) % (eyes_test_labels.shape[0] - batch)\n",
    "        batch_data = eyes_test_data[offset:(offset + batch), :, :]\n",
    "        batch_labels = eyes_test_labels[offset:(offset + batch), :]\n",
    "        #   Dictionary to be fed to TensorFlow Session\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, dropout: 0.93}\n",
    "        _, l, predictions = session.run([optimizer, loss, test_prediction], feed_dict=feed_dict)\n",
    "        #   Calculating the Accuracy of the predictions\n",
    "        accu = accuracy(predictions, batch_labels)\n",
    "        if (step % 100 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accu)\n",
    "        average += accu\n",
    "    print \"Average Testing Accuracy : \", (average / num_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
